{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f30eb04",
   "metadata": {},
   "source": [
    "## This is the rough code. \n",
    "- Please read it thoroughly to understand the whole process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615d537e",
   "metadata": {},
   "source": [
    "### PROJECT OUTLINE:\n",
    "- I am going to scrape: https://github.com/topics website\n",
    "- Getting different topics from this page. For each topic gather: Topic Title, Topic page URL,\n",
    "  Topic description\n",
    "- For each topic, the titles of top 25 repositories in the topic from the \n",
    "  topic page\n",
    "- For each Repository, the Repository Name, the Username who posted the\n",
    "  Repository, No. of stars and the repository URL would be gathered.\n",
    "- For each Topics a CSV file would be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3239091e",
   "metadata": {},
   "source": [
    "###  2. : Use the requests library to download web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e680a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --upgrade --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b8f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6aa81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_url = 'https://github.com/topics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eaa0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(topics_url)      # Downloads the webpage and saves it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2920bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code               # Status_code=(200-299) means the request was successfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "300731c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129374"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59992a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n<!DOCTYPE html>\\n<html lang=\"en\" data-color-mode=\"auto\" data-light-theme=\"light\" data-dark-theme=\"dark\">\\n  <head>\\n    <meta charset=\"utf-8\">\\n  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">\\n\\n\\n\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-B/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_contents = response.text\n",
    "page_contents[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "decbf1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    " with open('webpage.html', 'w') as f:\n",
    "        f.write(page_contents)           # Writes the Html data into a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c998b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0db78b29",
   "metadata": {},
   "source": [
    "### 3. :  Use Beautiful Soup to parse and extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0caca818",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --upgrade --quiet    # Installing beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6cdbbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73492433",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_contents, 'html.parser')   # Create an instance of BeautifulSoup class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3d804",
   "metadata": {},
   "source": [
    " Getting the Inspect code from webpage, in which we can see:\n",
    "- Each topics are under the '<p class=.....> \"topic-name </p>\n",
    "- Therefore, we find every P-tag and check if it has topic names.\n",
    "- To filter out only relevant topic names, we search for class=\"f3 lh-condensed mb-0 mt-1 Link--primary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "509e9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_title_tags = soup.find_all('p')    # Finding all occurences of \"p-tags\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dab67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "topic_title_tags = soup.find_all('p', {'class' : selection_class}) # Filtering and searching for class as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "987da145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"f3 lh-condensed mb-0 mt-1 Link--primary\">3D</p>,\n",
       " <p class=\"f3 lh-condensed mb-0 mt-1 Link--primary\">Ajax</p>,\n",
       " <p class=\"f3 lh-condensed mb-0 mt-1 Link--primary\">Algorithm</p>,\n",
       " <p class=\"f3 lh-condensed mb-0 mt-1 Link--primary\">Amp</p>,\n",
       " <p class=\"f3 lh-condensed mb-0 mt-1 Link--primary\">Android</p>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_title_tags[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092db06",
   "metadata": {},
   "source": [
    "- Getting Topic descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7ee5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_selector = \"f5 color-text-secondary mb-0 mt-1\"\n",
    "topic_desc_tags = soup.find_all('p', {'class' : desc_selector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81b6b4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"f5 color-text-secondary mb-0 mt-1\">\n",
       "               3D modeling is the process of virtually developing the surface and structure of a 3D object.\n",
       "             </p>,\n",
       " <p class=\"f5 color-text-secondary mb-0 mt-1\">\n",
       "               Ajax is a technique for creating interactive web applications.\n",
       "             </p>,\n",
       " <p class=\"f5 color-text-secondary mb-0 mt-1\">\n",
       "               Algorithms are self-contained sequences that carry out a variety of tasks.\n",
       "             </p>,\n",
       " <p class=\"f5 color-text-secondary mb-0 mt-1\">\n",
       "               Amp is a non-blocking concurrency framework for PHP.\n",
       "             </p>,\n",
       " <p class=\"f5 color-text-secondary mb-0 mt-1\">\n",
       "               Android is an operating system built by Google designed for mobile devices.\n",
       "             </p>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_desc_tags[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd7254",
   "metadata": {},
   "source": [
    "- Finding the URL of each topic and storing it\n",
    "- The link for each topics are in <a>...<\\a> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b326909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_link_tags = soup.find_all('a', class_ = 'd-flex no-underline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3eee938b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/topics/3d'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_link_tags[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ead1cbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/topics/3d\n"
     ]
    }
   ],
   "source": [
    "topic_0_link = \"https://github.com\" + topic_link_tags[0]['href']\n",
    "print(topic_0_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5099cc1a",
   "metadata": {},
   "source": [
    "- Getting only Title Text from Tags, and storing it in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53346f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3D'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_title_tags[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7e9e1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android', 'Angular', 'Ansible', 'API', 'Arduino', 'ASP.NET', 'Atom', 'Awesome Lists', 'Amazon Web Services', 'Azure', 'Babel', 'Bash', 'Bitcoin', 'Bootstrap', 'Bot', 'C', 'Chrome', 'Chrome extension', 'Command line interface', 'Clojure', 'Code quality', 'Code review', 'Compiler', 'Continuous integration', 'COVID-19', 'C++']\n"
     ]
    }
   ],
   "source": [
    "topic_titles = []\n",
    "for tags in topic_title_tags:\n",
    "    topic_titles.append(tags.text)\n",
    "print(topic_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc780372",
   "metadata": {},
   "source": [
    "- For Getting Desc text from desc tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9fdf2095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3D modeling is the process of virtually developing the surface and structure of a 3D object.', 'Ajax is a technique for creating interactive web applications.', 'Algorithms are self-contained sequences that carry out a variety of tasks.', 'Amp is a non-blocking concurrency framework for PHP.', 'Android is an operating system built by Google designed for mobile devices.']\n"
     ]
    }
   ],
   "source": [
    "topic_descs = []\n",
    "for tags in topic_desc_tags:\n",
    "    topic_descs.append(tags.text.strip())\n",
    "print(topic_descs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e890c39",
   "metadata": {},
   "source": [
    "- For getting only relevant urls from each url tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "258c99bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://github.com/topics/3d',\n",
       " 'https://github.com/topics/ajax',\n",
       " 'https://github.com/topics/algorithm',\n",
       " 'https://github.com/topics/amphp',\n",
       " 'https://github.com/topics/android']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_urls = []\n",
    "base_url = 'https://github.com'\n",
    "for tags in topic_link_tags:\n",
    "    topic_urls.append(base_url + tags['href'])\n",
    "topic_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3743995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c27b623",
   "metadata": {},
   "source": [
    "### 4. : Create CSV file(s) with the extracted information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a1f34",
   "metadata": {},
   "source": [
    "- Using Pandas Lib to create a table and write it to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "906bbed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1a83c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_dict = {\n",
    "    'title': topic_titles,\n",
    "    'descriptions' : topic_descs,\n",
    "    'topic_url' : topic_urls,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f518500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(topics_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df4f23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df.to_csv('github_topics.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8bbf7c",
   "metadata": {},
   "source": [
    "### 5 : Getting Info out of a topic Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ca1aeafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://github.com/topics/android'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_page_url = topic_urls[4]\n",
    "topic_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "097caf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(topic_page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c9b994ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e996122c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "581100"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6aa00610",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopic_soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6efbfab",
   "metadata": {},
   "source": [
    "- Since The Username & repository of the project are both 'a- tags', and both are stored in a superclass 'h1-tag', I'll find for the h-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a7ba3aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h1_repo_tags = subtopic_soup.find_all('h1', {'class': 'f3 color-text-secondary text-normal lh-condensed' } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a77e08f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = h1_repo_tags[0].find_all('a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "05218b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mrdoob'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tags[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7c002bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'three.js'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tags[1].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6843f83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/mrdoob/three.js\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://github.com'\n",
    "repo_url = base_url + a_tags[1]['href']\n",
    "print(repo_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3514ef",
   "metadata": {},
   "source": [
    "- Now for number of Stars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4a1e12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_finder_class = 'social-count float-none'\n",
    "star_tags = subtopic_soup.find_all('a', class_ = star_finder_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5261d8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(star_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "400d748b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'71.8k'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_tags[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "042e78ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(ele):\n",
    "    ele = ele.strip()\n",
    "    if ele[-1] == 'k':\n",
    "        return int(float(ele[:-1])*1000)\n",
    "    return int(ele)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "40c41f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71800"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_star_count(star_tags[0].text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89370c9",
   "metadata": {},
   "source": [
    "- Creating a function to take:\n",
    "   - h1 tag\n",
    "   - star_tag  and \n",
    "- return username, repo_name, no. of stars and repo url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff7bd6",
   "metadata": {},
   "source": [
    "- To summarise:\n",
    "- There are 30 h1 tags \n",
    "- each h1 tag has 1- a tag for username and 1-a tag for repo name & url\n",
    "- There are 30 star_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "857e1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h1_tag, star_tag):\n",
    "    a_tags = h1_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f6593ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mrdoob', 'three.js', 71800, 'https://github.com/mrdoob/three.js')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_repo_info(h1_repo_tags[0], star_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed9d46f",
   "metadata": {},
   "source": [
    "- Getting values of all the sub_topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "afde4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_repos_dict = {\n",
    "    'username' : [],\n",
    "    'repo_name' : [],\n",
    "    'stars' : [],\n",
    "    'repo_url' : [],\n",
    "}\n",
    "for i in range(len(h1_repo_tags)):\n",
    "    repo_info = get_repo_info(h1_repo_tags[i], star_tags[i])\n",
    "    topic_repos_dict['username'].append(repo_info[0])\n",
    "    topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "    topic_repos_dict['stars'].append(repo_info[2])\n",
    "    topic_repos_dict['repo_url'].append(repo_info[3])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2aaa0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_repos_df = pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5ca66",
   "metadata": {},
   "source": [
    "## Now, Collecting up the code in a few functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "11a00a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtopic_page(topic_urls):\n",
    "    #Download the page\n",
    "    response = requests.get(topic_urls)\n",
    "    # Check successful response\n",
    "    if response.status_code not in range(200,300):\n",
    "        raise Exception(\"Failed to load page {}\".format(topic_urls))\n",
    "    # Parse web-data using Beautiful_Soup\n",
    "    subtopic_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return subtopic_soup\n",
    "\n",
    "def get_repo_info(h1_tag, star_tag):\n",
    "    # Returns all the required info about the repository\n",
    "    a_tags = h1_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, stars, repo_url\n",
    "\n",
    "\n",
    "def get_subtopic_repos(subtopic_soup):\n",
    "    # gets the h1-tags containing username, repo_name & repo_url\n",
    "    h1_finder_class = 'f3 color-text-secondary text-normal lh-condensed'\n",
    "    h1_repo_tags = subtopic_soup.find_all('h1', {'class': h1_finder_class})\n",
    "    # gets the star_tags containing the no of stars in each repos\n",
    "    star_finder_class = 'social-count float-none'\n",
    "    star_tags = subtopic_soup.find_all('a', class_ = star_finder_class)\n",
    "    \n",
    "    # Gets required data for each repository under the main topic and stores\n",
    "    # it as a dictionary\n",
    "    topic_repos_dict = {\n",
    "        'username' : [],\n",
    "        'repo_name' : [],\n",
    "        'stars' : [],\n",
    "        'repo_url' : [],\n",
    "    }\n",
    "    \n",
    "    for i in range(len(h1_repo_tags)):\n",
    "        repo_info = get_repo_info(h1_repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "    \n",
    "    return pd.DataFrame(topic_repos_dict)   \n",
    "\n",
    "def scrape_subtopic(topic_url, topic_name):\n",
    "    subtopic_df = get_subtopic_repos(get_subtopic_page(topic_urls))\n",
    "    subtopic_df.to_csv(topic_name + '.csv', index=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce143a",
   "metadata": {},
   "source": [
    "- Get the list of top 30 Topics from GitHub site\n",
    "- Get the top 30 repos from each topic pages\n",
    "- For each topic, Create CSV file of the top repos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70f615",
   "metadata": {},
   "source": [
    "### FINAL CODE :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "52334067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "def get_subtopic_page(topic_urls):\n",
    "    #Download the page\n",
    "    response = requests.get(topic_urls)\n",
    "    # Check successful response\n",
    "    if response.status_code not in range(200,300):\n",
    "        raise Exception(\"Failed to load page {}\".format(topic_urls))\n",
    "    # Parse web-data using Beautiful_Soup\n",
    "    subtopic_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return subtopic_soup\n",
    "\n",
    "def get_repo_info(h1_tag, star_tag):\n",
    "    # Returns all the required info about the repository\n",
    "    a_tags = h1_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, stars, repo_url\n",
    "\n",
    "\n",
    "def get_subtopic_repos(subtopic_soup):\n",
    "    # gets the h1-tags containing username, repo_name & repo_url\n",
    "    h1_finder_class = 'f3 color-text-secondary text-normal lh-condensed'\n",
    "    h1_repo_tags = subtopic_soup.find_all('h1', {'class': h1_finder_class})\n",
    "    # gets the star_tags containing the no of stars in each repos\n",
    "    star_finder_class = 'social-count float-none'\n",
    "    star_tags = subtopic_soup.find_all('a', class_ = star_finder_class)\n",
    "    \n",
    "    # Gets required data for each repository under the main topic and stores\n",
    "    # it as a dictionary\n",
    "    topic_repos_dict = {\n",
    "        'username' : [],\n",
    "        'repo_name' : [],\n",
    "        'stars' : [],\n",
    "        'repo_url' : [],\n",
    "    }\n",
    "    \n",
    "    for i in range(len(h1_repo_tags)):\n",
    "        repo_info = get_repo_info(h1_repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "    \n",
    "    return pd.DataFrame(topic_repos_dict)   \n",
    "\n",
    "def scrape_subtopic(topic_url, path):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"The file {path} already exists. Skipping...\")\n",
    "        return\n",
    "    subtopic_df = get_subtopic_repos(get_subtopic_page(topic_url))\n",
    "    subtopic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "78259414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(soup):\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = soup.find_all('p', {'class' : selection_class})\n",
    "    topic_titles = []\n",
    "    for tags in topic_title_tags:\n",
    "        topic_titles.append(tags.text)\n",
    "    return topic_titles\n",
    "def get_topic_descs(soup):\n",
    "    desc_selector = \"f5 color-text-secondary mb-0 mt-1\"\n",
    "    topic_desc_tags = soup.find_all('p', {'class' : desc_selector})\n",
    "    topic_descs = []\n",
    "    for tags in topic_desc_tags:\n",
    "        topic_descs.append(tags.text.strip())\n",
    "    return topic_descs\n",
    "def get_topic_urls(soup):\n",
    "    topic_link_tags = soup.find_all('a', class_ = 'd-flex no-underline')\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    for tags in topic_link_tags:\n",
    "        topic_urls.append(base_url + tags['href'])\n",
    "    return topic_urls  \n",
    "        \n",
    "    \n",
    "def  scrape_topics():\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code not in range(200,300):\n",
    "        raise Exception(\"Failed to load page {}\".format(topic_url))\n",
    "    page_contents = response.text\n",
    "    soup = BeautifulSoup(page_contents, 'html.parser')\n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(soup),\n",
    "        'descriptions' : get_topic_descs(soup),\n",
    "        'topic_url' : get_topic_urls(soup),\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "cee53caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_url = 'https://github.com/topics'\n",
    "def scrape_topics_repos():\n",
    "    # Creating a separate folder = 'scraped_data(github)'\n",
    "    os.makedirs('scraped_data(github)', exist_ok = True)\n",
    "    print(f\"Scraping list of topics from {topics_url}\")\n",
    "    topics_df = scrape_topics()\n",
    "    for index,row in topics_df.iterrows():\n",
    "        print(f\"Scraping top repositories for {row['title']}\")\n",
    "        scrape_subtopic(row['topic_url'], f\"scraped_data(github)/{row['title']}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f962bf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrape_topics_repos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
